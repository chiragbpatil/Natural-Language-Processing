{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable,Union,List,Tuple,Literal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self,\n",
    "                 max_features:int=None,\n",
    "                 stop_words:Union[str,List[str]]='english',\n",
    "                 analyze:Literal[\"word\", \"char\"] = 'word',\n",
    "                 ngram_range:Tuple[int,int]=(1,1),\n",
    "                 lowercase:bool=True,\n",
    "                 binary:bool=False\n",
    "                 ):\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            \n",
    "            max_features (int): If not None, build a vocabulary that only consider the top\n",
    "                               `max_features` ordered by term frequency across the corpus.\n",
    "                                Otherwise, all features are used.\n",
    "                                Defaults to None.\n",
    "                                \n",
    "            stop_words (List[str]): list of words which will be considered as stop word and \n",
    "                                    will be removed from vocabulary.\n",
    "                                    If `english`, a built-in stop word list for English is used from NLTK.\n",
    "                                    Defaults to english.\n",
    "            \n",
    "            ngram_range (Tuple, optional): The lower and upper boundary of the range of n-values for different word\n",
    "                                           n-grams or char n-grams to be extracted.\n",
    "                                           All values of n such such that min_n <= n <= max_n will be used.\n",
    "                                           For example an ngram_range of (1, 1) means only unigrams, \n",
    "                                           (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "                                           Defaults to (1,1).\n",
    "            \n",
    "            analyzer (Literal[\"word\", \"char\"]) : Whether the feature should be made of word n-gram or character n-grams.\n",
    "                                                 Defaults to `word`.\n",
    "                                                \n",
    "            \n",
    "            lowercase (bool, optional): Convert all characters to lowercase before tokenizing.\n",
    "                                        Defaults to True.\n",
    "            \n",
    "            binary (bool, optional): if True calculate binary bagofword else count vectorizer.\n",
    "                                        Defaults to False.\n",
    "        \"\"\"  \n",
    "        self.ngram_range = ngram_range\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.analyze = analyze \n",
    "        self.max_features = max_features\n",
    "        self.binary = binary\n",
    "        \n",
    "        max_feature_validation = (isinstance(self.max_features,int) and self.max_features > 0) or self.max_features is None\n",
    "        stop_words_validation = isinstance(stop_words,list) or stop_words=='english'\n",
    "        ngram_range_validation = isinstance(self.ngram_range,tuple) or isinstance(ngram_range,list)\n",
    "        analyze_validation = self.analyze in ['word','char']\n",
    "        \n",
    "        if not max_feature_validation: raise ValueError(f\"max_features must be None or int and non zero, got {self.max_features}\")\n",
    "        if not analyze_validation: raise ValueError(f\"analyze must word or char, got {self.analyze}\")\n",
    "        if not stop_words_validation: raise ValueError(f\"stop_words must be `english` of list of words, got {self.stop_words}\")\n",
    "        if not ngram_range_validation: raise ValueError(f\"ngram_range must be tupe of list of intger like (1,2), got {ngram_range}\")\n",
    "        if not ngram_range[0] <= ngram_range[1]: assert ValueError(f\"ngram_range lower boundary must be lessthan or equal to upper boundary, got {self.ngram_range}\")\n",
    "                    \n",
    "        if self.stop_words=='english':\n",
    "            self.stop_words = STOP_WORDS\n",
    "            \n",
    "    def _extract_ngrams(self,tokens):\n",
    "        \n",
    "        # https://www.projectpro.io/recipes/find-ngrams-from-text\n",
    "        for num in range(self.ngram_range[0] ,self.ngram_range[1]+1):\n",
    "            \n",
    "            # we already have onegram which is tokens it self\n",
    "            # hence start with 2\n",
    "            if num > 1:\n",
    "                n_grams = ngrams(tokens, num)\n",
    "                n_grams = [ ' '.join(grams) for grams in n_grams]\n",
    "                tokens.extend(n_grams)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _generate_tokens(self,corpus:str):\n",
    "        \n",
    "        if self.lowercase:\n",
    "            corpus = corpus.lower()\n",
    "\n",
    "        if self.analyze == \"word\":\n",
    "            # convert text/input into word tokens\n",
    "            tokens = word_tokenize(corpus)\n",
    "            \n",
    "            tokens = self._extract_ngrams(tokens)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            input =  re.sub(' +','',corpus)\n",
    "            \n",
    "            input = input.strip()\n",
    "            \n",
    "            tokens = list(input)\n",
    "            \n",
    "            tokens = self._extract_ngrams(tokens)\n",
    "            \n",
    "        # remove stop words from tokens\n",
    "        final_tokens = [w for w in tokens if not w.lower() in self.stop_words]\n",
    "        \n",
    "        return final_tokens\n",
    "    \n",
    "    def _verify_and_return_input(self,input):\n",
    "        \n",
    "        if isinstance(input,str):\n",
    "            if not os.path.exists(input): raise FileNotFoundError(f\"{input} was not found or is a directory\")\n",
    "            with open(input,'r') as f:\n",
    "                input = f.read().splitlines()\n",
    "\n",
    "        else:\n",
    "            if not (isinstance(input,list) or isinstance(input,tuple)) : raise ValueError(f\"input must be list or tuple or filepath, got {input}\")\n",
    "        \n",
    "        return input\n",
    "                \n",
    "    def fit(self, input:Union[str,List[str]]):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            input (Union[str,List[str]]): input can be filepath or list of string\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab_word_count = pd.DataFrame()\n",
    "        \n",
    "        input = self._verify_and_return_input(input)\n",
    "        \n",
    "        input = \" \".join(input)\n",
    "                        \n",
    "        tokens = self._generate_tokens(input)\n",
    "        \n",
    "        for i,token in enumerate(set(tokens)):\n",
    "            self.vocab_word_count.loc[i,'word'] = token\n",
    "            self.vocab_word_count.loc[i,'count'] = tokens.count(token)\n",
    "        \n",
    "        self.vocab_word_count  =  self.vocab_word_count.sort_values(by=['count'], ascending=False).reset_index(drop = True)\n",
    "        \n",
    "        if self.max_features is not None:\n",
    "            self.vocab_word_count = self.vocab_word_count.loc[:self.max_features-1,:] \n",
    "            \n",
    "            \n",
    "    def transform(self,input:Union[str,List[str]])->np.array:\n",
    "        \"\"\"will calculate and return bag of word vector \n",
    "\n",
    "        Args:\n",
    "            input (Union[str,List[str]]): input can be filepath or list of string\n",
    "\n",
    "        Returns:\n",
    "            np.array: bagofword representation\n",
    "        \"\"\"        \"\"\"\"\"\"\n",
    "\n",
    "        input = self._verify_and_return_input(input)\n",
    "        \n",
    "        bag_of_word_arr = np.zeros((len(input),\n",
    "                                         self.vocab_word_count.shape[0])\n",
    "                                        )\n",
    "        for i,text in enumerate(input):\n",
    "            current_tokens = self._generate_tokens(text)\n",
    "            for ct in current_tokens:\n",
    "                index_of_word = self.vocab_word_count.index[self.vocab_word_count['word']==ct].tolist()\n",
    "                if len(index_of_word) > 0:\n",
    "                    bag_of_word_arr[i,index_of_word[0]] =  1 if self.binary else current_tokens.count(ct)\n",
    "        \n",
    "        return bag_of_word_arr\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test based on list of string\n",
    "bow_vectorizer = BagOfWords(max_features=10,ngram_range=(1,1),analyze='word')\n",
    "bow_vectorizer.fit(corpus)\n",
    "bow_arr = bow_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0., 0., 0.],\n",
       "       [2., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test based on file\n",
    "bow_vectorizer = BagOfWords(max_features=10,ngram_range=(1,1),analyze='word')\n",
    "bow_vectorizer.fit('corpus.txt')\n",
    "bow_arr = bow_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0., 0., 0.],\n",
       "       [2., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
