{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable,Union,List,Tuple,Literal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}$\n",
    "\n",
    "\n",
    "\n",
    "$IDF(t) = \\log_{e}\\frac{\\text{Total number of documents}}\n",
    "{\\text{Number of documents with term t in it}+1} + 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self,\n",
    "                 max_features:int=None,\n",
    "                 stop_words:Union[str,List[str]]='english',\n",
    "                 analyze:Literal[\"word\", \"char\"] = 'word',\n",
    "                 ngram_range:Tuple[int,int]=(1,1),\n",
    "                 lowercase:bool=True,\n",
    "                 binary:bool=False\n",
    "                 ):\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            \n",
    "            max_features (int): If not None, build a vocabulary that only consider the top\n",
    "                               `max_features` ordered by term frequency across the corpus.\n",
    "                                Otherwise, all features are used.\n",
    "                                Defaults to None.\n",
    "                                \n",
    "            stop_words (List[str]): list of words which will be considered as stop word and \n",
    "                                    will be removed from vocabulary.\n",
    "                                    If `english`, a built-in stop word list for English is used from NLTK.\n",
    "                                    Defaults to english.\n",
    "            \n",
    "            ngram_range (Tuple, optional): The lower and upper boundary of the range of n-values for different word\n",
    "                                           n-grams or char n-grams to be extracted.\n",
    "                                           All values of n such such that min_n <= n <= max_n will be used.\n",
    "                                           For example an ngram_range of (1, 1) means only unigrams, \n",
    "                                           (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "                                           Defaults to (1,1).\n",
    "            \n",
    "            analyzer (Literal[\"word\", \"char\"]) : Whether the feature should be made of word n-gram or character n-grams.\n",
    "                                                 Defaults to `word`.\n",
    "                                                \n",
    "            \n",
    "            lowercase (bool, optional): Convert all characters to lowercase before tokenizing.\n",
    "                                        Defaults to True.\n",
    "            \n",
    "            binary (bool, optional): if True calculate binary bagofword else count vectorizer.\n",
    "                                        Defaults to False.\n",
    "        \"\"\"  \n",
    "        self.ngram_range = ngram_range\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.analyze = analyze \n",
    "        self.max_features = max_features\n",
    "        self.binary = binary\n",
    "        \n",
    "        max_feature_validation = (isinstance(self.max_features,int) and self.max_features > 0) or self.max_features is None\n",
    "        stop_words_validation = isinstance(stop_words,list) or stop_words=='english'\n",
    "        ngram_range_validation = isinstance(self.ngram_range,tuple) or isinstance(ngram_range,list)\n",
    "        analyze_validation = self.analyze in ['word','char']\n",
    "        \n",
    "        if not max_feature_validation: raise ValueError(f\"max_features must be None or int and non zero, got {self.max_features}\")\n",
    "        if not analyze_validation: raise ValueError(f\"analyze must word or char, got {self.analyze}\")\n",
    "        if not stop_words_validation: raise ValueError(f\"stop_words must be `english` of list of words, got {self.stop_words}\")\n",
    "        if not ngram_range_validation: raise ValueError(f\"ngram_range must be tupe of list of intger like (1,2), got {ngram_range}\")\n",
    "        if not ngram_range[0] <= ngram_range[1]: assert ValueError(f\"ngram_range lower boundary must be lessthan or equal to upper boundary, got {self.ngram_range}\")\n",
    "                    \n",
    "        if self.stop_words=='english':\n",
    "            self.stop_words = STOP_WORDS\n",
    "            \n",
    "    def _extract_ngrams(self,tokens):\n",
    "        \n",
    "        # https://www.projectpro.io/recipes/find-ngrams-from-text\n",
    "        for num in range(self.ngram_range[0] ,self.ngram_range[1]+1):\n",
    "            \n",
    "            # we already have onegram which is tokens it self\n",
    "            # hence start with 2\n",
    "            if num > 1:\n",
    "                n_grams = ngrams(tokens, num)\n",
    "                n_grams = [ ' '.join(grams) for grams in n_grams]\n",
    "                tokens.extend(n_grams)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _generate_tokens(self,corpus:str):\n",
    "        \n",
    "        if self.lowercase:\n",
    "            corpus = corpus.lower()\n",
    "\n",
    "        if self.analyze == \"word\":\n",
    "            # convert text/input into word tokens\n",
    "            tokens = word_tokenize(corpus)\n",
    "            \n",
    "            tokens = self._extract_ngrams(tokens)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            input =  re.sub(' +','',corpus)\n",
    "            \n",
    "            input = input.strip()\n",
    "            \n",
    "            tokens = list(input)\n",
    "            \n",
    "            tokens = self._extract_ngrams(tokens)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        final_tokens = [w for w in tokens if not w.lower() in self.stop_words]\n",
    "        \n",
    "        return final_tokens\n",
    "    \n",
    "    def _verify_and_return_input(self,input):\n",
    "        \n",
    "        if isinstance(input,str):\n",
    "            if not os.path.exists(input): raise FileNotFoundError(f\"{input} was not found or is a directory\")\n",
    "            with open(input,'r') as f:\n",
    "                input = f.read().splitlines()\n",
    "\n",
    "        else:\n",
    "            if not (isinstance(input,list) or isinstance(input,tuple)) : raise ValueError(f\"input must be list or tuple or filepath, got {input}\")\n",
    "        \n",
    "        return input\n",
    "                \n",
    "    def fit(self, input:Union[str,List[str]]):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            input (Union[str,List[str]]): input can be filepath or list of string\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab_word_idf = pd.DataFrame()\n",
    "        \n",
    "        input = self._verify_and_return_input(input)\n",
    "        \n",
    "        total_document = len(input) \n",
    "        \n",
    "        joined_input = \" \".join(input)\n",
    "                        \n",
    "        tokens = self._generate_tokens(joined_input)\n",
    "        \n",
    "        for i,token in enumerate(set(tokens)):\n",
    "            \n",
    "            num_of_document_with_token = 1\n",
    "            \n",
    "            for doc in input:\n",
    "                if token in doc:\n",
    "                    num_of_document_with_token += 1\n",
    "            \n",
    "                    \n",
    "            self.vocab_word_idf.loc[i,'word'] = token\n",
    "            self.vocab_word_idf.loc[i,'idf'] = 1 + math.log( (total_document) / (num_of_document_with_token))\n",
    "            \n",
    "        \n",
    "        self.vocab_word_idf  =  self.vocab_word_idf.sort_values(by=['idf'], ascending=False).reset_index(drop = True)\n",
    "        \n",
    "        if self.max_features is not None:\n",
    "            self.vocab_word_idf = self.vocab_word_idf.loc[:self.max_features-1,:] \n",
    "            \n",
    "            \n",
    "    def transform(self,input:Union[str,List[str]])->np.array:\n",
    "        \"\"\"will calculate and return bag of word vector \n",
    "\n",
    "        Args:\n",
    "            input (Union[str,List[str]]): input can be filepath or list of string\n",
    "\n",
    "        Returns:\n",
    "            np.array: bagofword representation\n",
    "        \"\"\"        \"\"\"\"\"\"\n",
    "\n",
    "        input = self._verify_and_return_input(input)\n",
    "\n",
    "        tfidf_arr = np.zeros((len(input),self.vocab_word_idf.shape[0]))\n",
    "        \n",
    "        for i,text in enumerate(input):\n",
    "            \n",
    "            current_tokens = self._generate_tokens(text)\n",
    "\n",
    "            num_current_tokens = len(word_tokenize(text))\n",
    "\n",
    "            for ct in current_tokens:\n",
    "                word_detail = self.vocab_word_idf.loc[self.vocab_word_idf['word']==ct]\n",
    "                \n",
    "                if word_detail.shape[0] > 0:\n",
    "                    idf_of_word = word_detail['idf'].tolist()[0]\n",
    "                    index_of_word = word_detail.index.tolist()[0]\n",
    "                    \n",
    "                    tf = current_tokens.count(ct) / num_current_tokens\n",
    "                    tfidf_arr[i,index_of_word] =  idf_of_word * tf\n",
    "        \n",
    "        return tfidf_arr\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     \"The cat in the hat\",\n",
    "     \"The cat ate the mouse\",\n",
    "     \"The mouse ran away from the cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test based on list of string\n",
    "tfidf_vectorizer = TFIDF(max_features=10,ngram_range=(1,1),analyze='word')\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "tfidf_arr = tfidf_vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.28109302 0.         0.         0.         0.14246359]\n",
      " [0.28109302 0.         0.         0.         0.2        0.14246359]\n",
      " [0.         0.         0.20078073 0.20078073 0.14285714 0.1017597 ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ate</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hat</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ran</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>away</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mouse</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word       idf\n",
       "0    ate  1.405465\n",
       "1    hat  1.405465\n",
       "2    ran  1.405465\n",
       "3   away  1.405465\n",
       "4  mouse  1.000000\n",
       "5    cat  0.712318"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocab_word_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
